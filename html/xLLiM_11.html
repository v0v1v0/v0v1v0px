<div class="container">

<table style="width: 100%;"><tr>
<td>sllim_inverse_map</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Inverse Mapping from sllim parameters
</h2>

<h3>Description</h3>

<p>This function computes the prediction of a new response from the estimation of the SLLiM model, returned by the function <code>sllim</code>.</p>


<h3>Usage</h3>

<pre><code class="language-R">sllim_inverse_map(y,theta,verb=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>An <code>D x N</code> matrix of input observations  with variables in rows and subjects on columns</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta</code></td>
<td>
<p>An object returned by the <code>sllim</code> function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p>Verbosity: print out the progression of the algorithm. If <code>verb=0</code>, there is no print, if <code>verb=1</code>, the progression is printed out. Default is 0.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function computes the prediction of a new response from the estimation of a SLLiM model, returned by the function <code>sllim</code>.
Indeed, if the inverse conditional density <code class="reqn">p(X | Y)</code> and the marginal density <code class="reqn">p(Y)</code> are defined according to a SLLiM model (as described in <code>xLLiM-package</code> and <code>sllim</code>), the forward conditional density <code class="reqn">p(Y | X)</code> can be deduced.
</p>
<p>Under SLLiM model, it is recalled that the inverse conditional <code class="reqn">p(X | Y)</code> is a mixture of Student regressions with parameters <code class="reqn">(c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code> and <code class="reqn">(\pi_k,\alpha_k)_{k=1}^K</code>. Interestingly, the forward conditional <code class="reqn">p(Y | X)</code> is also a mixture of Student regressions with parameters <code class="reqn">(c_k^*,\Gamma_k^*,A_k^*,b_k^*,\Sigma_k^*)_{k=1}^K</code> and <code class="reqn">(\pi_k,\alpha_k)_{k=1}^K</code>. These parameters have a closed-form expression depending only on <code class="reqn">(c_k,\Gamma_k,A_k,b_k,\Sigma_k)_{k=1}^K</code> and <code class="reqn">(\pi_k,\alpha_k)_{k=1}^K</code>. 
</p>
<p>Finally, the forward density (of interest) has the following expression:
</p>
<p style="text-align: center;"><code class="reqn">p(Y | X=x) = \sum_k \frac{\pi_k S(x; c_k^*,\Gamma_k^*,\alpha_k,1)}{\sum_j \pi_j S(x; c_j^*,\Gamma_j^*,\alpha_j,1)} S(y; A_k^*x + b_k^*,\Sigma_k^*,\alpha_k^y,\gamma_k^y)</code>
</p>

<p>where <code class="reqn">(\alpha_k^y,\gamma_k^y)</code> determine the heaviness of the tail of the Generalized Student distribution.
Note that <code class="reqn">\alpha_k^y= \alpha_k + D/2</code>  and <code class="reqn">\gamma_k^y= 1 + 1/2 \delta(x,c_k^*,\Gamma_k^*)</code> where <code class="reqn">\delta</code> is the Mahalanobis distance. A prediction of a new vector of responses is computed by:
</p>
<p style="text-align: center;"><code class="reqn">E (Y | X=x) = \sum_k \frac{\pi_k S(x; c_k^*,\Gamma_k^*,\alpha_k,1)}{\sum_j \pi_j S(x; c_j^*,\Gamma_j^*,\alpha_j,1)} (A_k^*x + b_k^*)</code>
</p>

<p>where <code class="reqn">x</code> is a new  vector of observed covariates.
</p>


<h3>Value</h3>

<p>Returns a list with the following elements:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x_exp</code></td>
<td>
<p>An <code>L x N</code> matrix of predicted responses by posterior mean. If <code class="reqn">L_w</code> latent factors are added to the model, the first <code class="reqn">Lt</code> rows (<code class="reqn">1:Lt</code>) are predictions of responses and rows <code class="reqn">(L_t+1):L</code> (recall that <code class="reqn">L=L_t+L_w</code>) are estimations of latent factors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Weights of the posterior Gaussian mixture model</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893–911, 2015.
</p>
<p>[2] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1–14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>


<h3>See Also</h3>

<p><code>xLLiM-package</code>,<code>sllim</code> 
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(data.xllim)

## Setting 5 components in the model
K = 5

## the model can be initialized by running an EM algorithm for Gaussian Mixtures (EMGM)
r = emgm(data.xllim, init=K); 
## and then the sllim model is estimated
responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns
mod = sllim(responses,covariates,in_K=K,in_r=r);

# Prediction on a test dataset
data(data.xllim.test)
pred = sllim_inverse_map(data.xllim.test,mod)
## Predicted responses
print(pred$x_exp)

</code></pre>


</div>