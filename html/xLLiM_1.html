<div class="container">

<table style="width: 100%;"><tr>
<td>xLLiM-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
High Dimensional Locally-Linear Mapping
</h2>

<h3>Description</h3>

<p>Provides a tool for non linear mapping (non linear regression) using a mixture of regression model and an inverse regression strategy. The methods include the GLLiM model (see Deleforge et al (2015) &lt;DOI:10.1007/s11222-014-9461-5&gt;) based on Gaussian mixtures and a robust version of GLLiM, named SLLiM (see Perthame et al (2016) &lt;https://hal.archives-ouvertes.fr/hal-01347455&gt;) based on a mixture of Generalized Student distributions. The methods also include BLLiM (see Devijver et al (2017) &lt;https://arxiv.org/abs/1701.07899&gt;) which is an extension of GLLiM with a sparse block diagonal structure for large covariance matrices (particularly interesting for transcriptomic data).
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> xLLiM</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 2.1</td>
</tr>
<tr>
<td style="text-align: left;">
Date: </td>
<td style="text-align: left;"> 2017-05-23</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p>The methods implemented in this package adress the following non-linear mapping issue:
</p>
<p style="text-align: center;"><code class="reqn"> E(Y | X=x) = g(x),</code>
</p>

<p>where <code class="reqn">Y</code> is a L-vector of multivariate responses and <code class="reqn">X</code> is a large D-vector of covariates' profiles such that <code class="reqn">D \gg L</code>. The methods implemented in this package aims at estimating the non linear regression function <code class="reqn">g</code>.
</p>
<p>First, the methods of this package are based on an inverse regression strategy. The inverse conditional relation <code class="reqn">p(X | Y)</code> is specified in a way that the forward relation of interest <code class="reqn">p(Y | X)</code> can be deduced in closed-from. The large number <code class="reqn">D</code> of covariates is handled by this inverse regression trick, which acts as a dimension reduction technique. The number of parameters to estimate is therefore drastically reduced.
</p>
<p>Second, we propose to approximate the non linear <code class="reqn">g</code> regression function by a piecewise affine function. Therefore, a hidden discrete variable <code class="reqn">Z</code> is introduced, in order to separate the space in <code class="reqn">K</code> regressions such that an affine model holds in each  region <code class="reqn">k</code> between responses Y and variables X:
</p>
<p style="text-align: center;"><code class="reqn">X = \sum_{k=1}^K I_{Z=k} (A_k Y + b_k + E_k)</code>
</p>

<p>where <code class="reqn">A_k</code> is a <code class="reqn">D \times L</code> matrix of coeffcients for regression <code class="reqn">k</code>, <code class="reqn">b_k</code> is a D-vector of intercepts and <code class="reqn">E_k</code> is a random noise. 
</p>
<p>All the models implemented in this package are based on mixture of regression models. The components of the mixture are Gaussian for GLLiM. SLLiM is a robust extension of GLLiM, based on Generalized Student mixtures. Indeed, Generalized Student distributions are heavy-tailed distributions which improve the robustness of the model compared to their Gaussian counterparts. BLLiM is an extension of GLLiM designed to provide an interpretable prediction tool for the analysis of transcriptomic data. It assumes a block diagonal dependence structure between covariates (genes) conditionally to the response. The block structure is automatically chosen among a collection of models using the slope heuristics.
</p>
<p>For both GLLiM and SLLiM, this package provides the possibility to add <code class="reqn">L_w</code> latent variables, when the responses are partially observed. In this situation, the vector <code class="reqn">Y=(T,W)</code> is split into an observed <code class="reqn">L_t</code>-vector <code class="reqn">T</code> and an unobserved <code class="reqn">L_w</code>-vector <code class="reqn">W</code>. The total size of the response is therefore <code class="reqn">L=L_t+L_w</code> where <code class="reqn">L_w</code> is chosen by the user. See [1] for details but this amounts to consider factors and allows to add structure in the large dimensional covariance matrices. The user must choose the number of mixtures components <code class="reqn">K</code> and, if needed, the number of latent factors <code class="reqn">L_w</code>. For small datasets (less than 100 observations), we suggest to select both <code class="reqn">(K,L_w)</code> by minimizing the BIC criterion. For larger datasets, we suggest to set <code class="reqn">L_w</code> using BIC while setting <code class="reqn">K</code> to an arbitrary value large enough to catch non linear relations between responses and covariates and small enough to have several observations (at least 10) in each clusters. Indeed, for large datasets, the number of clusters should not have a strong impact on the results provided it is sufficiently large.
</p>
<p>We propose to assess the prediction accuracy of a new response <code class="reqn">x_{test}</code> by computing the NRMSE (Normalized Root Mean Square Error) which is the RMSE normalized by the RMSE of prediction by the mean of training responses:
</p>
<p style="text-align: center;"><code class="reqn">NRMSE = \frac{|| \hat{y} - x_{test}||_2}{|| \bar{y} - x_{test} ||_2}</code>
</p>

<p>where <code class="reqn">\hat{y}</code> is the predicted response, <code class="reqn">x_{test}</code> is the true testing response and <code class="reqn">\bar{y}</code> is the mean of training responses. 
</p>
<p>The functions available in this package are used in this order: 
</p>

<ul>
<li>
<p> Step 1 (optional): Initialization of the algorithm using  a Multivariate Gaussian mixture model and an EM algorithm implemented in the <code>emgm</code> function. Responses and covariates must be concatenated as described in the documentation of <code>emgm</code> which corresponds to a joint Gaussian Mixture Model (see Qiao et al, 2009).
</p>
</li>
<li>
<p> Step 2: Estimation of a regression model using one of the available models (<code>gllim</code>, <code>sllim</code> or <code>bllim</code>). User must specify the following arguments
</p>

<ul>
<li>
<p> for GLLiM or SLLiM: constraint on the large covariance matrices of covariates named <code class="reqn">\Sigma_k</code>. These matrices can be supposed diagonal and homoskedastic (isotropic) by setting <code>cstr=list(Sigma="i")</code> which is the default. Other constraints are diagonal and heteroskedastic <code>(Sigma="d")</code>, full matrix <code>(Sigma="")</code> or full but equal for each class <code>(Sigma="*")</code>. Except for the last constraint, in all previous constraints the matrices have their own parameterization.
</p>
</li>
<li>
<p> number of components <code class="reqn">K</code> in the  model. 
</p>
</li>
<li>
<p> for GLLiM or SLLiM: if needed, number of latent factors <code class="reqn">L_w</code>
</p>
</li>
</ul>
</li>
<li>
<p> Step 3: Prediction of responses for a testing dataset using the <code>gllim_inverse_map</code> or <code>sllim_inverse_map</code> functions. 
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Emeline Perthame (emeline.perthame@inria.fr), Florence Forbes (florence.forbes@inria.fr), Antoine Deleforge (antoine.deleforge@inria.fr)
</p>


<h3>References</h3>

<p>[1] A. Deleforge, F. Forbes, and R. Horaud. High-dimensional regression with Gaussian mixtures and partially-latent response variables. Statistics and Computing, 25(5):893–911, 2015.
</p>
<p>[2] E. Devijver, M. Gallopin, E. Perthame. Nonlinear network-based quantitative trait prediction from transcriptomic data. Submitted, 2017, available at <a href="https://arxiv.org/abs/1701.07899">https://arxiv.org/abs/1701.07899</a>.
</p>
<p>[3] E. Perthame, F. Forbes, and A. Deleforge. Inverse regression approach to robust nonlinear high-to-low dimensional mapping. Journal of Multivariate Analysis, 163(C):1–14, 2018. https://doi.org/10.1016/j.jmva.2017.09.009
</p>
<p>[4] X. Qiao and N. Minematsu. Mixture of probabilistic linear regressions: A unified view of GMM-based mapping techiques. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2009.
</p>
<p>The <code>gllim</code> and <code>gllim_inverse_map</code> functions have been converted to R from the original Matlab code of the GLLiM toolbox available on: <a href="https://team.inria.fr/perception/gllim_toolbox/">https://team.inria.fr/perception/gllim_toolbox/</a>
</p>


<h3>See Also</h3>

<p><code>shock-package</code>,<code>capushe-package</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">### Not run

## Example of inverse regression with GLLiM model
# data(data.xllim)  
# dim(data.xllim) #  size 52 y 100
# responses = data.xllim[1:2,] # 2 responses in rows and 100 observations in columns
# covariates = data.xllim[3:52,] # 50 covariates in rows and 100 observations in columns

## Set 5 components in the model
# K = 5

## Step 1: initialization of the posterior probabilities (class assignments) 
## via standard EM for a joint Gaussian Mixture Model
# r = emgm(rbind(responses, covariates), init=K); 

## Step 2: estimation of the model
## Default Lw=0 and cstr$Sigma="i"
# mod = gllim(responses,covariates,in_K=K,in_r=r)

## Skip Step 1 and go to Step 2: automatic initialization and estimation of the model
# mod = gllim(responses,covariates,in_K=K)

## Alternative: Add Lw=1 latent factor to the model
# mod = gllim(responses,covariates,in_K=K,in_r=r,Lw=1)

## Different constraints on the large covariance matrices can be added: 
## see details in the documentation of the GLLiM function
## description
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="i")) #default
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="d"))
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma=""))
# mod = gllim(responses,covariates,in_K=K,in_r=r,cstr=list(Sigma="*"))
## End of example of inverse regression with GLLiM model

## Step 3: Prediction on a test dataset
# data(data.xllim.test) size 50 y 20
# pred = gllim_inverse_map(data.xllim.test,mod)
## Predicted responses using the mean of \eqn{p(y | x)}.
# pred$x_exp

## Example of leave-ntest-out (1 fold cross-validation) procedure 
# n = ncol(covariates)
# ntest=10
# id.test = sample(1:n,ntest)
# train.responses = responses[,-id.test]
# train.covariates = covariates[,-id.test]
# test.responses = responses[,id.test]
# test.covariates = covariates[,id.test]

## Learn the model on training data
# mod = gllim(train.responses, train.covariates,in_K=K)

## Predict responses on testing data 
# pred = gllim_inverse_map(test.covariates,mod)$x_exp

## nrmse : normalized root mean square error to measure prediction performance 
## the normalization term is the rmse of the prediction by the mean of training responses
## an nrmse larger than 1 means that the procedure performs worse than prediction by the mean
# norm_term = sqrt(rowMeans(sweep(test.responses,1,rowMeans(train.responses),"-")^2))
## Returns 1 value for each response variable 
# nrmse = sqrt(rowMeans((test.responses-pred)^2))/norm_term 

</code></pre>


</div>